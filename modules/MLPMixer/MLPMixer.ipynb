{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "论文地址：https://arxiv.org/abs/2010.11929",
   "id": "be0a661dc58befa3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MLPMixer 的原理：\n",
    "\n",
    "![MLPMixer](./MLPMixer.png)"
   ],
   "id": "35f12e6cfceb780a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Flax 中 `einops.rearrange(x, 'n h w c -> n (h w) c')` 的作用和 PyTorch 中 `x.view(n, h * w, c)` 的作用相同，即将输入的张量 x 重塑为 (n, h * w, c) 形状。\n",
    "如果要改成 PyTorch 中模块的形式，可以写为 `nn.Flatten(start_dim=1, end_dim=2)(x)`。"
   ],
   "id": "9956dcc03715e951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import einops"
   ],
   "id": "dbc17ce9ab6f4410"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T06:03:34.044437Z",
     "start_time": "2024-05-08T06:03:31.759947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 3, 224, 224)\n",
    "x = nn.Flatten(start_dim=1, end_dim=2)(x)\n",
    "print(x.shape)"
   ],
   "id": "f1d459a1d31eed05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 672, 224])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T06:03:37.188017Z",
     "start_time": "2024-05-08T06:03:34.046527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 3, 224, 224)\n",
    "x = einops.rearrange(x, 'n h w c -> n (h w) c')\n",
    "print(x.shape)"
   ],
   "id": "cd7d35f363d4bc0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 672, 224])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T06:03:48.757829Z",
     "start_time": "2024-05-08T06:03:48.734754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 3, 224, 224)\n",
    "x = x.view(2, -1, 224)\n",
    "print(x.shape)"
   ],
   "id": "c005538d76280ff6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 672, 224])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MLP-Mixer即可以靠channel-mixing MLPs层结合不同channels的信息，也可以靠token-mixing MLPs层结合不同空间位置的信息。\n",
    "\n",
    "CNN的特点是inductive bias，ViT靠大量数据(JFT-300数据集)使性能战胜了CNN，说明大量的数据是可以战胜inductive bias的，这个MLP-Mixer也是一样。卷积相当于是一种认为设计的学习模式：即局部假设。能够以天然具备学习相邻信息的优势，但长远看来，在数据和算力提升的前提下，相比于attention甚至MLP，可能成为了限制。因为不用滑窗，也不用attention的方法其实是CNN的母集。\n",
    "\n",
    "早起人们放弃MLP而使用CNN的原因是算力不足，CNN更节省算力，训练好模型更容易。现在算力资源提高了，就有了重新回到MLP的可能。MLP-Mixer说明在分类这种简单的任务上是可以通过算力的堆砌来训练出比CNN更广义的MLP模型 (CNN可以看做是狭义的MLP)。\n",
    "\n",
    "最后，channel-mixing MLPs层相当于1×1 convolution，而token-mixing MLPs层相当于广义的depth-wise convolution，只是MLP-Mixer让这两种类型的层交替执行了。"
   ],
   "id": "954b38ec1fd308be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
